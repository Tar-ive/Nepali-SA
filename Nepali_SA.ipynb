{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoPEBPVwRd79PNA8htB+V6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/Nepali-SA/blob/SA/Nepali_SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3pf2TQBtzZ6",
        "outputId": "deb494c8-406f-4bcd-8059-fa4a3a9eefef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.9/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('sentiment_analysis_nepali_final.csv')\n"
      ],
      "metadata": {
        "id": "FLRXwmf5uqh6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-deva\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IipR-H_Tux1r",
        "outputId": "01888ee9-3d21-416d-8522-de93013fc2e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-deva-extra fonts-gargi fonts-lohit-deva fonts-nakula fonts-sahadeva\n",
            "  fonts-samyak-deva fonts-sarai\n",
            "The following NEW packages will be installed:\n",
            "  fonts-deva fonts-deva-extra fonts-gargi fonts-lohit-deva fonts-nakula\n",
            "  fonts-sahadeva fonts-samyak-deva fonts-sarai\n",
            "0 upgraded, 8 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 1,117 kB of archives.\n",
            "After this operation, 4,775 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-deva-extra all 3.0-5 [577 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-lohit-deva all 2.95.4-4 [78.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-sahadeva all 1.0-4 [136 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-nakula all 1.0-3 [167 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-samyak-deva all 1.2.2-4 [65.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-deva all 2:1.2 [2,918 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-gargi all 2.0-4 [42.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-sarai all 1.0-2 [47.4 kB]\n",
            "Fetched 1,117 kB in 1s (1,511 kB/s)\n",
            "Selecting previously unselected package fonts-deva-extra.\n",
            "(Reading database ... 128288 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-deva-extra_3.0-5_all.deb ...\n",
            "Unpacking fonts-deva-extra (3.0-5) ...\n",
            "Selecting previously unselected package fonts-lohit-deva.\n",
            "Preparing to unpack .../1-fonts-lohit-deva_2.95.4-4_all.deb ...\n",
            "Unpacking fonts-lohit-deva (2.95.4-4) ...\n",
            "Selecting previously unselected package fonts-sahadeva.\n",
            "Preparing to unpack .../2-fonts-sahadeva_1.0-4_all.deb ...\n",
            "Unpacking fonts-sahadeva (1.0-4) ...\n",
            "Selecting previously unselected package fonts-nakula.\n",
            "Preparing to unpack .../3-fonts-nakula_1.0-3_all.deb ...\n",
            "Unpacking fonts-nakula (1.0-3) ...\n",
            "Selecting previously unselected package fonts-samyak-deva.\n",
            "Preparing to unpack .../4-fonts-samyak-deva_1.2.2-4_all.deb ...\n",
            "Unpacking fonts-samyak-deva (1.2.2-4) ...\n",
            "Selecting previously unselected package fonts-deva.\n",
            "Preparing to unpack .../5-fonts-deva_2%3a1.2_all.deb ...\n",
            "Unpacking fonts-deva (2:1.2) ...\n",
            "Selecting previously unselected package fonts-gargi.\n",
            "Preparing to unpack .../6-fonts-gargi_2.0-4_all.deb ...\n",
            "Unpacking fonts-gargi (2.0-4) ...\n",
            "Selecting previously unselected package fonts-sarai.\n",
            "Preparing to unpack .../7-fonts-sarai_1.0-2_all.deb ...\n",
            "Unpacking fonts-sarai (1.0-2) ...\n",
            "Setting up fonts-sahadeva (1.0-4) ...\n",
            "Setting up fonts-lohit-deva (2.95.4-4) ...\n",
            "Setting up fonts-nakula (1.0-3) ...\n",
            "Setting up fonts-gargi (2.0-4) ...\n",
            "Setting up fonts-deva-extra (3.0-5) ...\n",
            "Setting up fonts-samyak-deva (1.2.2-4) ...\n",
            "Setting up fonts-sarai (1.0-2) ...\n",
            "Setting up fonts-deva (2:1.2) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 1\n",
        "    elif sentiment < 0:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "HsqytVvLu1OO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment_Analysis'] = data['Sentences'].apply(analyze_sentiment)\n"
      ],
      "metadata": {
        "id": "hUMSV_SEu4my"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('updated_csv_file.csv', index=False)\n"
      ],
      "metadata": {
        "id": "tLrzsqkKvCEM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment_Analysis'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxvTEtxyvLub",
        "outputId": "5cb62ff0-c01a-4b7d-d986-b195fc9b728c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: Sentiment_Analysis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ntlk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv64SwGjvbkP",
        "outputId": "b0c30146-822a-42e9-ff1a-668849048011"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sentiment_analysis_nepali_final.csv')\n"
      ],
      "metadata": {
        "id": "n6IjcirRvRLn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTKsyHsyxQKX",
        "outputId": "a21d4621-13dc-4d9a-9ff9-88e9c01c5ac4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                          Sentences  Sentiment\n",
            "0           0  म एक शिक्षक , शिक्षा क्षेत्रमा रमाएको मान्छे ।...          1\n",
            "1           1  म सरकारी स्कूल/कलेजमा पढेर करीब १२ बर्ष भन्दा ...          1\n",
            "2           2  कति राम्रो शिव मन्दिर देख्न पाइयो कुन ठाउको हो...          1\n",
            "3           3  मारुनी भन्ने वितिकै सामान्य नाचनीमा आधारित कथा...          1\n",
            "4           4  यो फ्लिम हेरिसकेपछी थाहा भयो कि किन दर्सकहरुले...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "O4DKEgd2vWq1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Tokenize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "train_text = vectorizer.fit_transform(train_data['Sentences'])\n",
        "test_text = vectorizer.transform(test_data['Sentences'])\n"
      ],
      "metadata": {
        "id": "mpdnuZEpw7qJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(train_text, train_data['Sentiment'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "oqA-JD8yxZDW",
        "outputId": "3e7b5cb3-8c19-487c-cf4d-b8f17b379416"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "predictions = model.predict(test_text)\n",
        "accuracy = accuracy_score(test_data['Sentiment'], predictions)\n",
        "precision = precision_score(test_data['Sentiment'], predictions)\n",
        "recall = recall_score(test_data['Sentiment'], predictions)\n",
        "f1 = f1_score(test_data['Sentiment'], predictions)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "PTacMtVBxeBt",
        "outputId": "aa419c5a-8fab-4fc2-b41a-a2537b0d97ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e16cd5231ec5>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2096\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \"\"\"\n\u001b[0;32m-> 2098\u001b[0;31m     _, r, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2099\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# load the data from the CSV file\n",
        "data = pd.read_csv('sentiment_analysis_nepali_final.csv')\n",
        "\n",
        "# split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# vectorize the text using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(train_data['Sentences'])\n",
        "test_vectors = vectorizer.transform(test_data['Sentences'])\n",
        "\n",
        "# train the classifier on the training data\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(train_vectors, train_data['Sentiment'])\n",
        "\n",
        "# make predictions on the test data\n",
        "predictions = classifier.predict(test_vectors)\n",
        "\n",
        "# calculate the accuracy, recall, and F1 score\n",
        "accuracy = accuracy_score(test_data['Sentiment'], predictions)\n",
        "recall = recall_score(test_data['Sentiment'], predictions, average='macro')\n",
        "f1 = f1_score(test_data['Sentiment'], predictions, average='macro')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Recall:', recall)\n",
        "print('F1 score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDL1gA1oxyKT",
        "outputId": "6e856071-84dd-4fac-aa39-e8bdee69478a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6229393685386979\n",
            "Recall: 0.5136497908627123\n",
            "F1 score: 0.5093272085627204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "S6OoawVtycge"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nepali_sentiment_model.pkl', 'wb') as f:\n",
        "    pickle.dump(classifier, f)"
      ],
      "metadata": {
        "id": "mO2-5wYUylIc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "# Save vectorizer to file\n",
        "with open('nepali_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "gGwcQBwoy5Qj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "with open('nepali_sentiment_model.pkl', 'rb') as file:\n",
        "    nb = pickle.load(file)\n",
        "\n",
        "# Load the saved vectorizer\n",
        "with open('nepali_vectorizer.pkl', 'rb') as file:\n",
        "    vectorizer = pickle.load(file)\n",
        "\n",
        "# Define a function to predict sentiment on a Nepali sentence\n",
        "def predict_sentiment(sentence):\n",
        "    sentence_vectorized = vectorizer.transform([sentence])\n",
        "    prediction = nb.predict(sentence_vectorized)\n",
        "    if prediction[0] == 1:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\"\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"मेरो साथीले मलाई रोमान्टिक गीत सुनाए।\"\n",
        "prediction1 = predict_sentiment(sentence1)\n",
        "print(prediction1)  # Output: Negative\n",
        "\n",
        "sentence2 = \"अध्ययन गर्न सकिन्न यो ठाउँ शान्त छैन। \"\n",
        "prediction2 = predict_sentiment(sentence2)\n",
        "print(prediction2)  # Output: Positive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjs2uNEyIWQ",
        "outputId": "693d3cc5-04ea-409d-d434-38fb8a316f34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import csv"
      ],
      "metadata": {
        "id": "w23LWaHO5MhN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(username):\n",
        "    url = \"http://www.twitter.com/\" + username\n",
        "    print(\"\\n\\nDownloading tweets for \" + username)\n",
        "    response = None\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except Exception as e:\n",
        "        print(repr(e))\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "    content = soup.findAll('div',{'class':'js-tweet-text-container'})\n",
        "    content = soup.findAll('p',{'class':'TweetTextSize js-tweet-text tweet-text'})\n",
        "    data=[]\n",
        "    for i in content:\n",
        "        value=i.text\n",
        "        value=re.sub(r'https?:\\/\\/.*[\\r\\n]*','',value)\n",
        "        value=re.sub(r'pic?.*','',value)\n",
        "        value=re.sub(r\"[a-zA-z.'#0-9@,:?'\\u200b\\u200c\\u200d!/&~-]\",'',value)\n",
        "        value=re.sub(r'[\"\"“”()’:;]','',value)\n",
        "        value=' '.join(value.split())\n",
        "        if value:\n",
        "            value=value.split(\"।\")          \n",
        "            for i in value:\n",
        "                if not i:\n",
        "                    pass\n",
        "                else:\n",
        "                    data.append(i)\n",
        "    return data\n",
        "data=extract(\"hamrorabi/status/947152762348781569\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcI9ZZJu5YOO",
        "outputId": "dcdd3074-f683-472e-9c69-239edf6d8c68"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Downloading tweets for hamrorabi/status/947152762348781569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "with open('data.csv', 'a',encoding=\"utf-8\") as writeFile:\n",
        "    writer = csv.writer(writeFile)\n",
        "    for i in data:\n",
        "        writer.writerow([i])"
      ],
      "metadata": {
        "id": "LrN4ELT65p-x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"./data.csv\")\n",
        "print(df.head(5))\n",
        "X=data['text']\n",
        "Y=list(data['label'].astype(\"int\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "UGoIxSmK5wIF",
        "outputId": "87204a44-1110-4781-d509-0a85111e0d5e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-553496708d0c>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Y=list(data['label'].astype(\"int\")\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv('sentiment_analysis_nepali_final.csv')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(data['Sentences'], data['Sentiment'], test_size=0.25)\n",
        "\n",
        "# Convert sentences to numerical feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(sentences_train)\n",
        "X_test = vectorizer.transform(sentences_test)\n",
        "\n",
        "# Train a machine learning model on the training set\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model's accuracy on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Model accuracy:', accuracy)\n",
        "\n",
        "# Use the model to predict sentiment on new sentences\n",
        "new_sentences = ['केहि ठिक छैन।', 'मैले यो फिल्म मन पर्यो।', 'यो किताब राम्रो छ।']\n",
        "X_new = vectorizer.transform(new_sentences)\n",
        "y_new = model.predict(X_new)\n",
        "print('Predicted sentiments:', y_new)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsOVZk608u-w",
        "outputId": "4c8c590a-c30f-4d18-d0f7-5c91b2061695"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.6191327670987931\n",
            "Predicted sentiments: [1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load the CSV dataset into a pandas dataframe\n",
        "df = pd.read_csv('sentiment_analysis_nepali_final.csv')\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    text = text.lower() # convert text to lowercase\n",
        "    text = re.sub('\\d', '', text) # remove digits\n",
        "    text = re.sub('\\n', '', text) # remove newline characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    text = word_tokenize(text) # tokenize the text\n",
        "    text = [word for word in text if word.isalpha()] # remove non-alphabetic tokens\n",
        "    text = [word for word in text if len(word) > 2] # remove short tokens\n",
        "    text = ' '.join(text) # join the tokens back into a string\n",
        "    return text\n",
        "\n",
        "df['Sentences'] = df['Sentences'].apply(preprocess)\n",
        "\n",
        "# Create the feature matrix and target vector\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(df['Sentences'])\n",
        "y = df['Sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the sentiment of the new sentences\n",
        "new_sentences = ['केहि ठिक छैन।', 'मैले यो फिल्म मन पर्यो।', 'यो किताब राम्रो छ।']\n",
        "new_sentences = [preprocess(sentence) for sentence in new_sentences]\n",
        "new_X = cv.transform(new_sentences)\n",
        "predicted_sentiments = clf.predict(new_X)\n",
        "\n",
        "print('Predicted sentiments:', predicted_sentiments)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8tW3NDz9mYr",
        "outputId": "a22e8e33-4e81-4dbd-c678-98da586fca96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentiments: [1 1 1]\n"
          ]
        }
      ]
    }
  ]
}